name: üè† Scraper Di√°rio de Im√≥veis

on:
  # Executar automaticamente todos os dias √†s 8h (hor√°rio UTC)
  # 8h UTC = 5h Bras√≠lia (considerando hor√°rio de ver√£o pode variar)
  schedule:
    - cron: '0 8 * * *'  # Todos os dias √†s 8h UTC
  
  # Permitir execu√ß√£o manual
  workflow_dispatch:
    inputs:
      debug_enabled:
        description: 'Habilitar debug'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'

jobs:
  scraper-imoveis:
    name: üîç Buscar Im√≥veis
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: üì• Checkout do c√≥digo
        uses: actions/checkout@v4
        
      - name: üêç Configurar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          
      - name: üì¶ Instalar depend√™ncias
        run: |
          python -m pip install --upgrade pip
          if [ -f "$GITHUB_WORKSPACE/requirements.txt" ]; then
            echo "üì¶ Instalando depend√™ncias de requirements.txt"
            pip install -r "$GITHUB_WORKSPACE/requirements.txt"
          else
            echo "‚ö†Ô∏è requirements.txt n√£o encontrado; instalando depend√™ncias m√≠nimas"
            pip install selenium pandas webdriver-manager beautifulsoup4 requests lxml || true
          fi
          
      - name: üñ•Ô∏è Instalar Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version
          
      - name: üîß Configurar ambiente
        run: |
          echo "PYTHONPATH=$GITHUB_WORKSPACE/src" >> $GITHUB_ENV
          echo "Ambiente configurado"
          
      - name: üè† Executar scraper
        id: scraper
        run: |
          echo "üöÄ Iniciando scraper de im√≥veis..."
          python scraper_automatico.py
        continue-on-error: true
        
      - name: üìä Verificar resultados
        id: check_results
        run: |
          echo "üìÅ Verificando arquivos gerados..."
          
          # Contar arquivos
          RELATORIOS=$(ls relatorio_*.txt 2>/dev/null | wc -l || echo 0)
          JSON_FILES=$(ls imoveis_*.json 2>/dev/null | wc -l || echo 0)
          CSV_FILES=$(ls imoveis_*.csv 2>/dev/null | wc -l || echo 0)
          SCREENSHOTS=$(ls screenshot_*.png 2>/dev/null | wc -l || echo 0)
          
          echo "relatorios=$RELATORIOS" >> $GITHUB_OUTPUT
          echo "json_files=$JSON_FILES" >> $GITHUB_OUTPUT
          echo "csv_files=$CSV_FILES" >> $GITHUB_OUTPUT
          echo "screenshots=$SCREENSHOTS" >> $GITHUB_OUTPUT
          
          echo "üìä Resultados encontrados:"
          echo "  - Relat√≥rios: $RELATORIOS"
          echo "  - Arquivos JSON: $JSON_FILES"
          echo "  - Arquivos CSV: $CSV_FILES"
          echo "  - Screenshots: $SCREENSHOTS"
          
          # Verificar se teve sucesso
          if [ $RELATORIOS -gt 0 ] || [ $JSON_FILES -gt 0 ]; then
            echo "‚úÖ Scraper executou com sucesso!"
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Nenhum arquivo gerado"
            echo "success=false" >> $GITHUB_OUTPUT
          fi
          
      - name: üì§ Upload dos resultados
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: resultados-scraper-${{ github.run_number }}
          path: |
            relatorio_*.txt
            imoveis_*.json
            imoveis_*.csv
            screenshot_*.png
            *.log
          retention-days: 30
          if-no-files-found: warn
          
      - name: üìà Resumo da execu√ß√£o
        if: always()
        run: |
          echo "## üìä Resumo da Execu√ß√£o do Scraper" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üïê **Data/Hora**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "üî¢ **Run**: #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "üì¶ **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìÅ Arquivos Gerados" >> $GITHUB_STEP_SUMMARY
          echo "- üìÑ Relat√≥rios: ${{ steps.check_results.outputs.relatorios }}" >> $GITHUB_STEP_SUMMARY
          echo "- üìä JSON: ${{ steps.check_results.outputs.json_files }}" >> $GITHUB_STEP_SUMMARY
          echo "- üìã CSV: ${{ steps.check_results.outputs.csv_files }}" >> $GITHUB_STEP_SUMMARY
          echo "- üì∏ Screenshots: ${{ steps.check_results.outputs.screenshots }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.check_results.outputs.success }}" = "true" ]; then
            echo "### ‚úÖ Status: SUCESSO" >> $GITHUB_STEP_SUMMARY
            echo "O scraper executou com sucesso e gerou resultados." >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ö†Ô∏è Status: ATEN√á√ÉO" >> $GITHUB_STEP_SUMMARY
            echo "O scraper executou mas n√£o gerou todos os arquivos esperados." >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: üîî Notifica√ß√£o de falha
        if: failure()
        run: |
          echo "‚ùå SCRAPER FALHOU!"
          echo "Verifique os logs para identificar o problema."
          exit 1

